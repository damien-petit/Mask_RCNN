{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import skimage.io\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "import lab\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "LAB_DIR = os.path.join(ROOT_DIR, \"lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = lab.class_names\n",
    "# class_names = [\n",
    "#     'BG', 'bontaname', 'caloriemate', 'caramel'\n",
    "# ]\n",
    "# class_names = [\n",
    "#     'BG', 'apollo', 'bontaname', 'caloriemate', 'caramel', 'chocoball',\n",
    "#     'highremon', 'macadamia', 'sanipe', 'strawberry', 'zeraisu'\n",
    "# ]\n",
    "# class_names = [\n",
    "#     'BG', 'blendy', 'buttercookies', 'caloriemate', 'chocopie', 'choice',\n",
    "#     'zeraisu', 'jagabee', 'levain', 'meltykiss', 'pocky'\n",
    "# ]\n",
    "# class_names = [\n",
    "#     'BG', 'caloriemate', 'meltykiss', 'zeraisu'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = lab.LabConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "#DEVICE = \"/cpu:0\"\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "# weights_path = LAB_DIR + \"/models/20200706/caloriemate/mask_rcnn_lab_0050.h5\"\n",
    "# weights_path = LAB_DIR + \"/models/20200723/one-class/mask_rcnn_lab_0100.h5\"\n",
    "weights_path = LAB_DIR + \"/models/20200707/one-class/mask_rcnn_lab_0100.h5\"\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "plot_model(model.keras_model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "Image('model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_DIR = os.path.join(LAB_DIR, \"test/caloriemate\")\n",
    "for file in sorted(os.listdir(TEST_DIR)):\n",
    "    if not file.endswith(\".png\") and not file.endswith(\".jpg\"): continue\n",
    "    image = skimage.io.imread(TEST_DIR + \"/\" + file)\n",
    "    \n",
    "    results = model.detect([image], verbose=1)\n",
    "    \n",
    "    r = results[0]\n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "augment_param = iaa.Affine(rotate=90)\n",
    "\n",
    "TEST_DIR = os.path.join(LAB_DIR, \"test/caloriemate\")\n",
    "for file in sorted(os.listdir(TEST_DIR)):\n",
    "    if not file.endswith(\".png\") and not file.endswith(\".jpg\"): continue\n",
    "    image = skimage.io.imread(TEST_DIR + \"/\" + file)\n",
    "    image = augment_param.augment_image(image)\n",
    "    \n",
    "    results = model.detect([image], verbose=1)\n",
    "    \n",
    "    r = results[0]\n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "fs = cv2.FileStorage(\"./intrinsic.xml\", cv2.FILE_STORAGE_READ)\n",
    "camera_matrix = fs.getNode(\"camera_matrix\").mat()\n",
    "dist_coeffs = fs.getNode(\"distortion_coefficients\").mat()\n",
    "\n",
    "# test_class = [\n",
    "#     'blendy', 'buttercookies', 'caloriemate', 'chocopie', 'choice',\n",
    "#     'zeraisu', 'jagabee', 'levain', 'meltykiss', 'pocky'\n",
    "# ]\n",
    "test_class = [\"caloriemate\"]\n",
    "    \n",
    "for class_name in test_class:   \n",
    "    TEST_DIR = os.path.join(LAB_DIR, \"test/\" + class_name)\n",
    "    SAVE_DIR = os.path.join(LAB_DIR, \"test/result/20200723/one-class_epoch100/\" + class_name + \"_undistorted_synthetic\")\n",
    "    \n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "    \n",
    "    for file in sorted(os.listdir(TEST_DIR)):\n",
    "        if not file.endswith(\".png\") and not file.endswith(\".jpg\"): continue\n",
    "    #     image = skimage.io.imread(TEST_DIR + \"/\" + file)\n",
    "\n",
    "    #     skimage.io.imsave(SAVE_DIR + \"/\" + file, image)\n",
    "\n",
    "        image = cv2.imread(TEST_DIR + \"/\" + file)    \n",
    "        cv2.imwrite(SAVE_DIR + \"/\" + file, image)\n",
    "\n",
    "        image = cv2.undistort(image, camera_matrix, dist_coeffs)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = model.detect([image], verbose=1)    \n",
    "        result = results[0]\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        plt.rcParams[\"figure.subplot.left\"] = 0\n",
    "        plt.rcParams[\"figure.subplot.bottom\"] = 0\n",
    "        plt.rcParams[\"figure.subplot.right\"] = 1\n",
    "        plt.rcParams[\"figure.subplot.top\"] = 1\n",
    "\n",
    "        fig = Figure(figsize=(width/100,height/100))\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        axes = fig.gca()\n",
    "        visualize.display_instances(image, result['rois'], result['masks'],\n",
    "                                    result['class_ids'], class_names,\n",
    "                                    result['scores'], ax=axes)\n",
    "\n",
    "        canvas.draw()\n",
    "        result = np.fromstring(canvas.tostring_rgb(), dtype='uint8')\n",
    "\n",
    "        _, _, w, h = fig.bbox.bounds\n",
    "        result = result.reshape((int(h), int(w), 3))\n",
    "\n",
    "        save_name = file.split(\".\")[0] + \"_res.png\"\n",
    "        skimage.io.imsave(SAVE_DIR + \"/\" + save_name, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mAP @ IoU=50 on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "dataset = lab.LabDataset()\n",
    "dataset.load_lab(LAB_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as utils.py function\n",
    "def compute_overlaps_masks(masks1, masks2):\n",
    "    \"\"\"Computes IoU overlaps between two sets of masks.\n",
    "    masks1, masks2: [Height, Width, instances]\n",
    "    \"\"\"\n",
    "    \n",
    "    # If either set of masks is empty return empty result\n",
    "    if masks1.shape[-1] == 0 or masks2.shape[-1] == 0:\n",
    "        return np.zeros((masks1.shape[-1], masks2.shape[-1]))\n",
    "    # flatten masks and compute their areas\n",
    "    masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)\n",
    "    masks2 = np.reshape(masks2 > .5, (-1, masks2.shape[-1])).astype(np.float32)\n",
    "    area1 = np.sum(masks1, axis=0)\n",
    "    area2 = np.sum(masks2, axis=0)\n",
    "    print(\"pred_mask\", masks1.shape)\n",
    "    print(\"gt_mask\", masks2.shape)\n",
    "    print(\"pred_mask_area\", area1.shape)\n",
    "    print(\"gt_mask_area\", area2.shape)\n",
    "\n",
    "    # intersections and union\n",
    "    intersections = np.dot(masks1.T, masks2)\n",
    "    print(\"intersections\", intersections.shape)\n",
    "    union = area1[:, None] + area2[None, :] - intersections\n",
    "    print(\"union\", union.shape)\n",
    "    overlaps = intersections / union\n",
    "    print(\"overlaps\", overlaps.shape)\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "def trim_zeros(x):\n",
    "    \"\"\"It's common to have tensors larger than the available data and\n",
    "    pad with zeros. This function removes rows that are all zeros.\n",
    "    x: [rows, columns].\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == 2\n",
    "    return x[~np.all(x == 0, axis=1)]\n",
    "\n",
    "def compute_matches(gt_boxes, gt_class_ids, gt_masks,\n",
    "                    pred_boxes, pred_class_ids, pred_scores, pred_masks,\n",
    "                    iou_threshold=0.5, score_threshold=0.0):\n",
    "    \"\"\"Finds matches between prediction and ground truth instances.\n",
    "    Returns:\n",
    "        gt_match: 1-D array. For each GT box it has the index of the matched\n",
    "                  predicted box.\n",
    "        pred_match: 1-D array. For each predicted box, it has the index of\n",
    "                    the matched ground truth box.\n",
    "        overlaps: [pred_boxes, gt_boxes] IoU overlaps.\n",
    "    \"\"\"\n",
    "    # Trim zero padding\n",
    "    # TODO: cleaner to do zero unpadding upstream\n",
    "    gt_boxes = trim_zeros(gt_boxes)\n",
    "    gt_masks = gt_masks[..., :gt_boxes.shape[0]]\n",
    "    pred_boxes = trim_zeros(pred_boxes)\n",
    "    pred_scores = pred_scores[:pred_boxes.shape[0]]\n",
    "    # Sort predictions by score from high to low\n",
    "    indices = np.argsort(pred_scores)[::-1]\n",
    "    pred_boxes = pred_boxes[indices]\n",
    "    pred_class_ids = pred_class_ids[indices]\n",
    "    pred_scores = pred_scores[indices]\n",
    "    pred_masks = pred_masks[..., indices]\n",
    "\n",
    "    # Compute IoU overlaps [pred_masks, gt_masks]\n",
    "    overlaps = compute_overlaps_masks(pred_masks, gt_masks)\n",
    "\n",
    "    # Loop through predictions and find matching ground truth boxes\n",
    "    match_count = 0\n",
    "    print(\"pred_boxes shape\", pred_boxes.shape)\n",
    "    pred_match = -1 * np.ones([pred_boxes.shape[0]])\n",
    "    gt_match = -1 * np.ones([gt_boxes.shape[0]])\n",
    "    for i in range(len(pred_boxes)):\n",
    "        # Find best matching ground truth box\n",
    "        # 1. Sort matches by score\n",
    "        sorted_ixs = np.argsort(overlaps[i])[::-1]\n",
    "        # 2. Remove low scores\n",
    "        low_score_idx = np.where(overlaps[i, sorted_ixs] < score_threshold)[0]\n",
    "        if low_score_idx.size > 0:\n",
    "            sorted_ixs = sorted_ixs[:low_score_idx[0]]\n",
    "        # 3. Find the match\n",
    "        for j in sorted_ixs:\n",
    "            # If ground truth box is already matched, go to next one\n",
    "            if gt_match[j] > -1:\n",
    "                continue\n",
    "            # If we reach IoU smaller than the threshold, end the loop\n",
    "            iou = overlaps[i, j]\n",
    "            if iou < iou_threshold:\n",
    "                break\n",
    "            # Do we have a match?\n",
    "            if pred_class_ids[i] == gt_class_ids[j]:\n",
    "                match_count += 1\n",
    "                gt_match[j] = i\n",
    "                pred_match[i] = j\n",
    "                break\n",
    "\n",
    "    return gt_match, pred_match, overlaps\n",
    "\n",
    "def compute_ap(gt_boxes, gt_class_ids, gt_masks,\n",
    "               pred_boxes, pred_class_ids, pred_scores, pred_masks,\n",
    "               iou_threshold=0.5):\n",
    "    \"\"\"Compute Average Precision at a set IoU threshold (default 0.5).\n",
    "    Returns:\n",
    "    mAP: Mean Average Precision\n",
    "    precisions: List of precisions at different class score thresholds.\n",
    "    recalls: List of recall values at different class score thresholds.\n",
    "    overlaps: [pred_boxes, gt_boxes] IoU overlaps.\n",
    "    \"\"\"\n",
    "    # Get matches and overlaps\n",
    "    gt_match, pred_match, overlaps = compute_matches(\n",
    "        gt_boxes, gt_class_ids, gt_masks,\n",
    "        pred_boxes, pred_class_ids, pred_scores, pred_masks,\n",
    "        iou_threshold)\n",
    "\n",
    "    # Compute precision and recall at each prediction box step\n",
    "    precisions = np.cumsum(pred_match > -1) / (np.arange(len(pred_match)) + 1)\n",
    "    recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n",
    "\n",
    "    # Pad with start and end values to simplify the math\n",
    "    precisions = np.concatenate([[0], precisions, [0]])\n",
    "    recalls = np.concatenate([[0], recalls, [1]])\n",
    "\n",
    "    # Ensure precision values decrease but don't increase. This way, the\n",
    "    # precision value at each recall threshold is the maximum it can be\n",
    "    # for all following recall thresholds, as specified by the VOC paper.\n",
    "    for i in range(len(precisions) - 2, -1, -1):\n",
    "        precisions[i] = np.maximum(precisions[i], precisions[i + 1])\n",
    "\n",
    "    # Compute mean AP over recall range\n",
    "    indices = np.where(recalls[:-1] != recalls[1:])[0] + 1\n",
    "    mAP = np.sum((recalls[indices] - recalls[indices - 1]) *\n",
    "                 precisions[indices])\n",
    "\n",
    "    return mAP, precisions, recalls, overlaps\n",
    "\n",
    "image_id = np.random.choice(dataset.image_ids)\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "results = model.detect([image], verbose=0)\n",
    "# Compute AP\n",
    "r = results[0]\n",
    "AP, precisions, recalls, overlaps =\\\n",
    "    compute_ap(gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'], r['scores'], r['masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_batch_ap(image_ids):\n",
    "    APs = []\n",
    "    for image_id in image_ids:\n",
    "        # Load image\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        # Compute AP\n",
    "        r = results[0]\n",
    "        AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                              r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "        APs.append(AP)\n",
    "    return APs\n",
    "\n",
    "# Pick a set of random images\n",
    "image_ids = np.random.choice(dataset.image_ids, 10)\n",
    "# image_ids = dataset.image_ids\n",
    "APs = compute_batch_ap(image_ids)\n",
    "print(\"mAP @ IoU=50: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "dataset = lab.LabDataset()\n",
    "dataset.load_lab(LAB_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.class_names = class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recalls = []\n",
    "for image_id in dataset.image_ids:\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id, use_mini_mask=False)    \n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    gt_masks = np.reshape(gt_mask > 0.5, (-1, gt_mask.shape[-1])).astype(np.float32)\n",
    "    pr_masks = np.reshape(r[\"masks\"] > 0.5, (-1, r[\"masks\"].shape[-1])).astype(np.float32)\n",
    "  \n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n",
    "    \n",
    "    matches = 0    \n",
    "    for gt in gt_masks.T:\n",
    "        ious = []\n",
    "        for pr in pr_masks.T:\n",
    "            overlap = np.sum(np.logical_and(gt, pr))\n",
    "            iou = overlap / np.sum(np.logical_or(gt, pr))\n",
    "            ious.append(iou)\n",
    "            \n",
    "        #print(\"max iou :\", max(ious))\n",
    "        if max(ious) > 0.7:\n",
    "            matches += 1\n",
    "            \n",
    "    recall = matches/gt_masks.shape[-1]\n",
    "    print(\"recall :\", recall)\n",
    "    recalls.append(recall)\n",
    "    \n",
    "print(recalls)\n",
    "print(\"Average recall\", np.mean(recalls))\n",
    "print(\"max recall\", np.max(recalls))\n",
    "print(\"min recall\", np.min(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0: Backbone (Residual Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.class_names = class_names\n",
    "\n",
    "# Load validation dataset\n",
    "dataset = lab.LabDataset()\n",
    "dataset.load_lab(LAB_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = random.choice(dataset.image_ids)\n",
    "image_id = 9\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = model.run_graph([image], [\n",
    "    (\"input_image\",              tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n",
    "    (\"zero_padding2d_1_input\",   model.keras_model.get_layer(\"zero_padding2d_1\").input), \n",
    "    (\"zero_padding2d_1_output\",  model.keras_model.get_layer(\"zero_padding2d_1\").output),\n",
    "    (\"conv1_input\",              model.keras_model.get_layer(\"conv1\").input), \n",
    "    (\"conv1_output\",             model.keras_model.get_layer(\"conv1\").output),\n",
    "    (\"bn_conv1_input\",           model.keras_model.get_layer(\"bn_conv1\").input),\n",
    "    (\"bn_conv1_output\",          model.keras_model.get_layer(\"bn_conv1\").output),\n",
    "    (\"activation_1_input\",       model.keras_model.get_layer(\"activation_1\").input),\n",
    "    (\"activation_1_output\",      model.keras_model.get_layer(\"activation_1\").output),\n",
    "    (\"max_pooling2d_1_input\",    model.keras_model.get_layer(\"max_pooling2d_1\").input),\n",
    "    (\"max_pooling2d_1_output\",   model.keras_model.get_layer(\"max_pooling2d_1\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2a = model.run_graph([image], [\n",
    "    (\"res2a_branch2a_input\",              model.keras_model.get_layer(\"res2a_branch2a\").input), \n",
    "    (\"res2a_branch2a_output\",             model.keras_model.get_layer(\"res2a_branch2a\").output),\n",
    "    (\"bn2a_branch2a_input\",           model.keras_model.get_layer(\"bn2a_branch2a\").input),\n",
    "    (\"bn2a_branch2a_output\",          model.keras_model.get_layer(\"bn2a_branch2a\").output),\n",
    "    (\"activation_2_input\",       model.keras_model.get_layer(\"activation_2\").input),\n",
    "    (\"activation_2_output\",      model.keras_model.get_layer(\"activation_2\").output),\n",
    "    (\"res2a_branch2b_input\",              model.keras_model.get_layer(\"res2a_branch2b\").input), \n",
    "    (\"res2a_branch2b_output\",             model.keras_model.get_layer(\"res2a_branch2b\").output),\n",
    "    (\"bn2a_branch2b_input\",           model.keras_model.get_layer(\"bn2a_branch2b\").input),\n",
    "    (\"bn2a_branch2b_output\",          model.keras_model.get_layer(\"bn2a_branch2b\").output),\n",
    "    (\"activation_3_input\",       model.keras_model.get_layer(\"activation_3\").input),\n",
    "    (\"activation_3_output\",      model.keras_model.get_layer(\"activation_3\").output),\n",
    "    (\"res2a_branch2c_input\",              model.keras_model.get_layer(\"res2a_branch2c\").input), \n",
    "    (\"res2a_branch2c_output\",             model.keras_model.get_layer(\"res2a_branch2c\").output),\n",
    "    (\"bn2a_branch2c_input\",           model.keras_model.get_layer(\"bn2a_branch2c\").input),\n",
    "    (\"bn2a_branch2c_output\",          model.keras_model.get_layer(\"bn2a_branch2c\").output),\n",
    "    \n",
    "    (\"res2a_branch1_input\",              model.keras_model.get_layer(\"res2a_branch1\").input), \n",
    "    (\"res2a_branch1_output\",             model.keras_model.get_layer(\"res2a_branch1\").output),\n",
    "    (\"bn2a_branch1_input\",           model.keras_model.get_layer(\"bn2a_branch1\").input),\n",
    "    (\"bn2a_branch1_output\",          model.keras_model.get_layer(\"bn2a_branch1\").output),\n",
    "    \n",
    "    (\"add_1_input\",              tf.identity(model.keras_model.get_layer(\"add_1\").input)), \n",
    "    (\"add_1_output\",             model.keras_model.get_layer(\"add_1\").output),\n",
    "    (\"res2a_out_input\",              model.keras_model.get_layer(\"res2a_out\").input), \n",
    "    (\"res2a_out_output\",             model.keras_model.get_layer(\"res2a_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2b = model.run_graph([image], [\n",
    "    (\"res2b_branch2a_input\",              model.keras_model.get_layer(\"res2b_branch2a\").input), \n",
    "    (\"res2b_branch2a_output\",             model.keras_model.get_layer(\"res2b_branch2a\").output),\n",
    "    (\"bn2b_branch2a_input\",           model.keras_model.get_layer(\"bn2b_branch2a\").input),\n",
    "    (\"bn2b_branch2a_output\",          model.keras_model.get_layer(\"bn2b_branch2a\").output),\n",
    "    (\"activation_4_input\",       model.keras_model.get_layer(\"activation_4\").input),\n",
    "    (\"activation_4_output\",      model.keras_model.get_layer(\"activation_4\").output),\n",
    "    (\"res2b_branch2b_input\",              model.keras_model.get_layer(\"res2b_branch2b\").input), \n",
    "    (\"res2b_branch2b_output\",             model.keras_model.get_layer(\"res2b_branch2b\").output),\n",
    "    (\"bn2b_branch2b_input\",           model.keras_model.get_layer(\"bn2b_branch2b\").input),\n",
    "    (\"bn2b_branch2b_output\",          model.keras_model.get_layer(\"bn2b_branch2b\").output),\n",
    "    (\"activation_5_input\",       model.keras_model.get_layer(\"activation_5\").input),\n",
    "    (\"activation_5_output\",      model.keras_model.get_layer(\"activation_5\").output),\n",
    "    (\"res2b_branch2c_input\",              model.keras_model.get_layer(\"res2b_branch2c\").input), \n",
    "    (\"res2b_branch2c_output\",             model.keras_model.get_layer(\"res2b_branch2c\").output),\n",
    "    (\"bn2b_branch2c_input\",           model.keras_model.get_layer(\"bn2b_branch2c\").input),\n",
    "    (\"bn2b_branch2c_output\",          model.keras_model.get_layer(\"bn2b_branch2c\").output),\n",
    "        \n",
    "    (\"add_2_input\",              tf.identity(model.keras_model.get_layer(\"add_2\").input)), \n",
    "    (\"add_2_output\",             model.keras_model.get_layer(\"add_2\").output),\n",
    "    (\"res2b_out_input\",              model.keras_model.get_layer(\"res2b_out\").input), \n",
    "    (\"res2b_out_output\",             model.keras_model.get_layer(\"res2b_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be long, let's check key layer onlly\n",
    "res2c = model.run_graph([image], [\n",
    "    (\"res2c_branch2a_input\",              model.keras_model.get_layer(\"res2c_branch2a\").input), \n",
    "    (\"res2c_branch2a_output\",             model.keras_model.get_layer(\"res2c_branch2a\").output),\n",
    "    #...\n",
    "    (\"res2c_branch2c_input\",              model.keras_model.get_layer(\"res2c_branch2c\").input), \n",
    "    (\"res2c_branch2c_output\",             model.keras_model.get_layer(\"res2c_branch2c\").output),\n",
    "    (\"bn2c_branch2c_input\",           model.keras_model.get_layer(\"bn2c_branch2c\").input),\n",
    "    (\"bn2c_branch2c_output\",          model.keras_model.get_layer(\"bn2c_branch2c\").output),\n",
    "        \n",
    "    (\"add_3_input\",              tf.identity(model.keras_model.get_layer(\"add_3\").input)), \n",
    "    (\"add_3_output\",             model.keras_model.get_layer(\"add_3\").output),\n",
    "    (\"res2c_out_input\",              model.keras_model.get_layer(\"res2c_out\").input), \n",
    "    (\"res2c_out_output\",             model.keras_model.get_layer(\"res2c_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3a = model.run_graph([image], [\n",
    "    (\"res3a_branch2a_input\",              model.keras_model.get_layer(\"res3a_branch2a\").input), \n",
    "    (\"res3a_branch2a_output\",             model.keras_model.get_layer(\"res3a_branch2a\").output),\n",
    "    #...\n",
    "    (\"res3a_branch2c_input\",              model.keras_model.get_layer(\"res3a_branch2c\").input), \n",
    "    (\"res3a_branch2c_output\",             model.keras_model.get_layer(\"res3a_branch2c\").output),\n",
    "    (\"bn3a_branch2c_input\",           model.keras_model.get_layer(\"bn3a_branch2c\").input),\n",
    "    (\"bn3a_branch2c_output\",          model.keras_model.get_layer(\"bn3a_branch2c\").output),\n",
    "    \n",
    "    (\"res3a_branch1_input\",              model.keras_model.get_layer(\"res3a_branch1\").input), \n",
    "    (\"res3a_branch1_output\",             model.keras_model.get_layer(\"res3a_branch1\").output),\n",
    "    (\"bn3a_branch1_input\",           model.keras_model.get_layer(\"bn3a_branch1\").input),\n",
    "    (\"bn3a_branch1_output\",          model.keras_model.get_layer(\"bn3a_branch1\").output),\n",
    "    \n",
    "    (\"fpn_c2p2_input\",           model.keras_model.get_layer(\"fpn_c2p2\").input),\n",
    "    (\"fpn_c2p2_output\",          model.keras_model.get_layer(\"fpn_c2p2\").output),\n",
    "    \n",
    "    (\"add_4_input\",              tf.identity(model.keras_model.get_layer(\"add_4\").input)), \n",
    "    (\"add_4_output\",             model.keras_model.get_layer(\"add_4\").output),\n",
    "    (\"res3a_out_input\",              model.keras_model.get_layer(\"res3a_out\").input), \n",
    "    (\"res3a_out_output\",             model.keras_model.get_layer(\"res3a_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3b = model.run_graph([image], [\n",
    "    (\"res3b_branch2a_input\",              model.keras_model.get_layer(\"res3b_branch2a\").input), \n",
    "    (\"res3b_branch2a_output\",             model.keras_model.get_layer(\"res3b_branch2a\").output),\n",
    "    #...\n",
    "    (\"res3b_branch2c_input\",              model.keras_model.get_layer(\"res3b_branch2c\").input), \n",
    "    (\"res3b_branch2c_output\",             model.keras_model.get_layer(\"res3b_branch2c\").output),\n",
    "    (\"bn3b_branch2c_input\",           model.keras_model.get_layer(\"bn3b_branch2c\").input),\n",
    "    (\"bn3b_branch2c_output\",          model.keras_model.get_layer(\"bn3b_branch2c\").output),\n",
    "    \n",
    "    (\"add_5_input\",              tf.identity(model.keras_model.get_layer(\"add_5\").input)), \n",
    "    (\"add_5_output\",             model.keras_model.get_layer(\"add_5\").output),\n",
    "    (\"res3b_out_input\",              model.keras_model.get_layer(\"res3b_out\").input), \n",
    "    (\"res3b_out_output\",             model.keras_model.get_layer(\"res3b_out\").output),\n",
    "])\n",
    "\n",
    "#res3c\n",
    "\n",
    "#res3d\n",
    "res3d = model.run_graph([image], [    \n",
    "    (\"add_7_input\",              tf.identity(model.keras_model.get_layer(\"add_7\").input)), \n",
    "    (\"add_7_output\",             model.keras_model.get_layer(\"add_7\").output),\n",
    "    (\"res3d_out_input\",              model.keras_model.get_layer(\"res3d_out\").input), \n",
    "    (\"res3d_out_output\",             model.keras_model.get_layer(\"res3d_out\").output),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res4a = model.run_graph([image], [\n",
    "    (\"res4a_branch2a_input\",              model.keras_model.get_layer(\"res4a_branch2a\").input), \n",
    "    (\"res4a_branch2a_output\",             model.keras_model.get_layer(\"res4a_branch2a\").output),\n",
    "    #...\n",
    "    (\"res4a_branch2c_input\",              model.keras_model.get_layer(\"res4a_branch2c\").input), \n",
    "    (\"res4a_branch2c_output\",             model.keras_model.get_layer(\"res4a_branch2c\").output),\n",
    "    (\"bn4a_branch2c_input\",           model.keras_model.get_layer(\"bn4a_branch2c\").input),\n",
    "    (\"bn4a_branch2c_output\",          model.keras_model.get_layer(\"bn4a_branch2c\").output),\n",
    "    \n",
    "    (\"res4a_branch1_input\",              model.keras_model.get_layer(\"res4a_branch1\").input), \n",
    "    (\"res4a_branch1_output\",             model.keras_model.get_layer(\"res4a_branch1\").output),\n",
    "    (\"bn4a_branch1_input\",           model.keras_model.get_layer(\"bn4a_branch1\").input),\n",
    "    (\"bn4a_branch1_output\",          model.keras_model.get_layer(\"bn4a_branch1\").output),\n",
    "    \n",
    "    (\"fpn_c3p3_input\",           model.keras_model.get_layer(\"fpn_c3p3\").input),\n",
    "    (\"fpn_c3p3_output\",          model.keras_model.get_layer(\"fpn_c3p3\").output),\n",
    "    \n",
    "    (\"add_8_input\",              tf.identity(model.keras_model.get_layer(\"add_8\").input)), \n",
    "    (\"add_8_output\",             model.keras_model.get_layer(\"add_8\").output),\n",
    "    (\"res4a_out_input\",              model.keras_model.get_layer(\"res4a_out\").input), \n",
    "    (\"res4a_out_output\",             model.keras_model.get_layer(\"res4a_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res4b = model.run_graph([image], [\n",
    "    (\"res4b_branch2a_input\",              model.keras_model.get_layer(\"res4b_branch2a\").input), \n",
    "    (\"res4b_branch2a_output\",             model.keras_model.get_layer(\"res4b_branch2a\").output),\n",
    "    #...\n",
    "    (\"res4b_branch2c_input\",              model.keras_model.get_layer(\"res4b_branch2c\").input), \n",
    "    (\"res4b_branch2c_output\",             model.keras_model.get_layer(\"res4b_branch2c\").output),\n",
    "    (\"bn4b_branch2c_input\",           model.keras_model.get_layer(\"bn4b_branch2c\").input),\n",
    "    (\"bn4b_branch2c_output\",          model.keras_model.get_layer(\"bn4b_branch2c\").output),\n",
    "    \n",
    "    (\"add_9_input\",              tf.identity(model.keras_model.get_layer(\"add_9\").input)), \n",
    "    (\"add_9_output\",             model.keras_model.get_layer(\"add_9\").output),\n",
    "    (\"res4b_out_input\",              model.keras_model.get_layer(\"res4b_out\").input), \n",
    "    (\"res4b_out_output\",             model.keras_model.get_layer(\"res4b_out\").output),\n",
    "])\n",
    "\n",
    "# res4c ~ res4w\n",
    "res4w = model.run_graph([image], [\n",
    "    (\"res4w_branch2a_input\",              model.keras_model.get_layer(\"res4w_branch2a\").input), \n",
    "    (\"res4w_branch2a_output\",             model.keras_model.get_layer(\"res4w_branch2a\").output),\n",
    "    #...\n",
    "    (\"res4w_branch2c_input\",              model.keras_model.get_layer(\"res4w_branch2c\").input), \n",
    "    (\"res4w_branch2c_output\",             model.keras_model.get_layer(\"res4w_branch2c\").output),\n",
    "    (\"bn4w_branch2c_input\",           model.keras_model.get_layer(\"bn4w_branch2c\").input),\n",
    "    (\"bn4w_branch2c_output\",          model.keras_model.get_layer(\"bn4w_branch2c\").output),\n",
    "    \n",
    "    (\"add_30_input\",              tf.identity(model.keras_model.get_layer(\"add_30\").input)), \n",
    "    (\"add_30_output\",             model.keras_model.get_layer(\"add_30\").output),\n",
    "    (\"res4w_out_input\",              model.keras_model.get_layer(\"res4w_out\").input), \n",
    "    (\"res4w_out_output\",             model.keras_model.get_layer(\"res4w_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res5a = model.run_graph([image], [\n",
    "    (\"res5a_branch2a_input\",              model.keras_model.get_layer(\"res5a_branch2a\").input), \n",
    "    (\"res5a_branch2a_output\",             model.keras_model.get_layer(\"res5a_branch2a\").output),\n",
    "    #...\n",
    "    (\"res5a_branch2c_input\",              model.keras_model.get_layer(\"res5a_branch2c\").input), \n",
    "    (\"res5a_branch2c_output\",             model.keras_model.get_layer(\"res5a_branch2c\").output),\n",
    "    (\"bn5a_branch2c_input\",           model.keras_model.get_layer(\"bn5a_branch2c\").input),\n",
    "    (\"bn5a_branch2c_output\",          model.keras_model.get_layer(\"bn5a_branch2c\").output),\n",
    "    \n",
    "    (\"res5a_branch1_input\",              model.keras_model.get_layer(\"res5a_branch1\").input), \n",
    "    (\"res5a_branch1_output\",             model.keras_model.get_layer(\"res5a_branch1\").output),\n",
    "    (\"bn5a_branch1_input\",           model.keras_model.get_layer(\"bn5a_branch1\").input),\n",
    "    (\"bn5a_branch1_output\",          model.keras_model.get_layer(\"bn5a_branch1\").output),\n",
    "    \n",
    "    (\"fpn_c4p4_input\",           model.keras_model.get_layer(\"fpn_c4p4\").input),\n",
    "    (\"fpn_c4p4_output\",          model.keras_model.get_layer(\"fpn_c4p4\").output),\n",
    "    \n",
    "    (\"add_31_input\",              tf.identity(model.keras_model.get_layer(\"add_31\").input)), \n",
    "    (\"add_31_output\",             model.keras_model.get_layer(\"add_31\").output),\n",
    "    (\"res5a_out_input\",              model.keras_model.get_layer(\"res5a_out\").input), \n",
    "    (\"res5a_out_output\",             model.keras_model.get_layer(\"res5a_out\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res5b = model.run_graph([image], [\n",
    "    (\"res5b_branch2a_input\",              model.keras_model.get_layer(\"res5b_branch2a\").input), \n",
    "    (\"res5b_branch2a_output\",             model.keras_model.get_layer(\"res5b_branch2a\").output),\n",
    "    #...\n",
    "    (\"res5b_branch2c_input\",              model.keras_model.get_layer(\"res5b_branch2c\").input), \n",
    "    (\"res5b_branch2c_output\",             model.keras_model.get_layer(\"res5b_branch2c\").output),\n",
    "    (\"bn5b_branch2c_input\",           model.keras_model.get_layer(\"bn5b_branch2c\").input),\n",
    "    (\"bn5b_branch2c_output\",          model.keras_model.get_layer(\"bn5b_branch2c\").output),\n",
    "    \n",
    "    (\"add_32_input\",              tf.identity(model.keras_model.get_layer(\"add_32\").input)), \n",
    "    (\"add_32_output\",             model.keras_model.get_layer(\"add_32\").output),\n",
    "    (\"res5b_out_input\",              model.keras_model.get_layer(\"res5b_out\").input), \n",
    "    (\"res5b_out_output\",             model.keras_model.get_layer(\"res5b_out\").output),\n",
    "])\n",
    "\n",
    "res5c = model.run_graph([image], [    \n",
    "    (\"res5c_out_input\",              model.keras_model.get_layer(\"res5c_out\").input), \n",
    "    (\"res5c_out_output\",             model.keras_model.get_layer(\"res5c_out\").output),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn = model.run_graph([image], [    \n",
    "    (\"fpn_c5p5_input\",           model.keras_model.get_layer(\"fpn_c5p5\").input),\n",
    "    (\"fpn_c5p5_output\",          model.keras_model.get_layer(\"fpn_c5p5\").output),    \n",
    "    (\"fpn_p5_input\",           model.keras_model.get_layer(\"fpn_p5\").input),\n",
    "    (\"fpn_p5_output\",          model.keras_model.get_layer(\"fpn_p5\").output),\n",
    "    (\"fpn_p6_input\",           model.keras_model.get_layer(\"fpn_p6\").input),\n",
    "    (\"fpn_p6_output\",          model.keras_model.get_layer(\"fpn_p6\").output),\n",
    "    (\"fpn_p5upsampled_input\",           model.keras_model.get_layer(\"fpn_p5upsampled\").input),\n",
    "    (\"fpn_p5upsampled_output\",          model.keras_model.get_layer(\"fpn_p5upsampled\").output),\n",
    "    \n",
    "    (\"fpn_p4add_input\",           tf.identity(model.keras_model.get_layer(\"fpn_p4add\").input)),\n",
    "    (\"fpn_p4add_output\",          model.keras_model.get_layer(\"fpn_p4add\").output),\n",
    "    (\"fpn_p4_input\",           model.keras_model.get_layer(\"fpn_p4\").input),\n",
    "    (\"fpn_p4_output\",          model.keras_model.get_layer(\"fpn_p4\").output),\n",
    "    (\"fpn_p4upsampled_input\",           model.keras_model.get_layer(\"fpn_p4upsampled\").input),\n",
    "    (\"fpn_p4upsampled_output\",          model.keras_model.get_layer(\"fpn_p4upsampled\").output),\n",
    "    \n",
    "    (\"fpn_p3add_input\",           tf.identity(model.keras_model.get_layer(\"fpn_p3add\").input)),\n",
    "    (\"fpn_p3add_output\",          model.keras_model.get_layer(\"fpn_p3add\").output),\n",
    "    (\"fpn_p3_input\",           model.keras_model.get_layer(\"fpn_p3\").input),\n",
    "    (\"fpn_p3_output\",          model.keras_model.get_layer(\"fpn_p3\").output),\n",
    "    (\"fpn_p3upsampled_input\",           model.keras_model.get_layer(\"fpn_p3upsampled\").input),\n",
    "    (\"fpn_p3upsampled_output\",          model.keras_model.get_layer(\"fpn_p3upsampled\").output),\n",
    "    \n",
    "    (\"fpn_p2add_input\",           tf.identity(model.keras_model.get_layer(\"fpn_p2add\").input)),\n",
    "    (\"fpn_p2add_output\",          model.keras_model.get_layer(\"fpn_p2add\").output),\n",
    "    (\"fpn_p2_input\",           model.keras_model.get_layer(\"fpn_p2\").input),\n",
    "    (\"fpn_p2_output\",          model.keras_model.get_layer(\"fpn_p2\").output),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Region Proposal Network\n",
    "\n",
    "The Region Proposal Network (RPN) runs a lightweight binary classifier on a lot of boxes (anchors) over the image and returns object/no-object scores. Anchors with high *objectness* score (positive anchors) are passed to the stage two to be classified.\n",
    "\n",
    "Often, even positive anchors don't cover objects fully. So the RPN also regresses a refinement (a delta in location and size) to be applied to the anchors to shift it and resize it a bit to the correct boundaries of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a RPN Targets\n",
    "The RPN targets are the training values for the RPN. To generate the targets, we start with a grid of anchors that cover the full image at different scales, and then we compute the IoU of the anchors with ground truth object. Positive anchors are those that have an IoU >= 0.7 with any ground truth object, and negative anchors are those that don't cover any object by more than 0.3 IoU. Anchors in between (i.e. cover an object by IoU >= 0.3 but < 0.7) are considered neutral and excluded from training.\n",
    "\n",
    "To train the RPN regressor, we also compute the shift and resizing needed to make the anchor cover the ground truth object completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab.class_names = class_names\n",
    "\n",
    "# Load validation dataset\n",
    "dataset = lab.LabDataset()\n",
    "dataset.load_lab(LAB_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_id = random.choice(dataset.image_ids)\n",
    "image_id = 9\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RPN trainig targets\n",
    "# target_rpn_match is 1 for positive anchors, -1 for negative anchors\n",
    "# and 0 for neutral anchors.\n",
    "target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\n",
    "    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\n",
    "log(\"target_rpn_match\", target_rpn_match)\n",
    "log(\"target_rpn_bbox\", target_rpn_bbox)\n",
    "\n",
    "positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\n",
    "negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\n",
    "neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\n",
    "positive_anchors = model.anchors[positive_anchor_ix]\n",
    "negative_anchors = model.anchors[negative_anchor_ix]\n",
    "neutral_anchors = model.anchors[neutral_anchor_ix]\n",
    "log(\"positive_anchors\", positive_anchors)\n",
    "log(\"negative_anchors\", negative_anchors)\n",
    "log(\"neutral anchors\", neutral_anchors)\n",
    "\n",
    "# Apply refinement deltas to positive anchors\n",
    "refined_anchors = utils.apply_box_deltas(\n",
    "    positive_anchors,\n",
    "    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\n",
    "log(\"refined_anchors\", refined_anchors, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display positive anchors before refinement (dotted) and\n",
    "# after refinement (solid).\n",
    "visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b RPN Predictions\n",
    "Here we run the RPN graph and display its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RPN sub-graph\n",
    "pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n",
    "\n",
    "# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\n",
    "nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
    "if nms_node is None:\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
    "if nms_node is None: #TF 1.9-1.10\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\n",
    "\n",
    "rpn = model.run_graph([image], [\n",
    "    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n",
    "    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
    "    (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
    "    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
    "    (\"post_nms_anchor_ix\", nms_node),\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show top anchors by score (before refinement)\n",
    "limit = 200\n",
    "sorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\n",
    "visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors with refinement. Then with clipping to image boundaries\n",
    "limit = 50\n",
    "ax = get_ax(1, 2)\n",
    "pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\n",
    "refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\n",
    "refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2])\n",
    "visualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n",
    "                     refined_boxes=refined_anchors[:limit], ax=ax[0])\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show refined anchors after non-max suppression\n",
    "limit = 50\n",
    "ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final proposals\n",
    "# These are the same as the previous step (refined anchors \n",
    "# after NMS) but with coordinates normalized to [0, 1] range.\n",
    "limit = 1000\n",
    "# Convert back to image coordinates for display\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\n",
    "visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the RPN recall (percent of objects covered by anchors)\n",
    "# Here we measure recall for 3 different methods:\n",
    "# - All anchors\n",
    "# - All refined anchors\n",
    "# - Refined anchors after NMS\n",
    "iou_threshold = 0.7\n",
    "\n",
    "recall, positive_anchor_ids = utils.compute_recall(model.anchors, gt_bbox, iou_threshold)\n",
    "print(\"All Anchors ({:5})       Recall: {:.3f}  Positive anchors: {}\".format(\n",
    "    model.anchors.shape[0], recall, len(positive_anchor_ids)))\n",
    "\n",
    "recall, positive_anchor_ids = utils.compute_recall(rpn['refined_anchors'][0], gt_bbox, iou_threshold)\n",
    "print(\"Refined Anchors ({:5})   Recall: {:.3f}  Positive anchors: {}\".format(\n",
    "    rpn['refined_anchors'].shape[1], recall, len(positive_anchor_ids)))\n",
    "\n",
    "recall, positive_anchor_ids = utils.compute_recall(proposals, gt_bbox, iou_threshold)\n",
    "print(\"Post NMS Anchors ({:5})  Recall: {:.3f}  Positive anchors: {}\".format(\n",
    "    proposals.shape[0], recall, len(positive_anchor_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Proposal Classification\n",
    "\n",
    "This stage takes the region proposals from the RPN and classifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Proposal Classification\n",
    "\n",
    "Run the classifier heads on proposals to generate class propbabilities and bounding box regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output to classifier and mask heads.\n",
    "mrcnn = model.run_graph([image], [\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n",
    "    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detection class IDs. Trim zero padding.\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\n",
    "# det_class_ids = det_class_ids[:det_count]\n",
    "detections = mrcnn['detections'][0, :]\n",
    "# detections = mrcnn['detections'][0, :det_count]\n",
    "detections = mrcnn['detections'][0, :]\n",
    "\n",
    "# print(\"{} detections: {}\".format(\n",
    "#     det_count, np.array(dataset.class_names)[det_class_ids]))\n",
    "print(\"{} detections: {}\".format(\n",
    "    det_count, np.array(class_names)[det_class_ids]))\n",
    "\n",
    "\n",
    "# captions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n",
    "#             for c, s in zip(detections[:, 4], detections[:, 5])]\n",
    "captions = [\"{} {:.3f}\".format(class_names[int(c)], s) if c > 0 else \"\"\n",
    "            for c, s in zip(detections[:, 4], detections[:, 5])]\n",
    "\n",
    "visualize.draw_boxes(\n",
    "    image, \n",
    "    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n",
    "    visibilities=[2] * len(detections),\n",
    "    captions=captions, title=\"Detections\",\n",
    "    ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Step by Step Detection\n",
    "\n",
    "Here we dive deeper into the process of processing the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = model.run_graph([image], [\n",
    "    (\"input_anchors_input\", tf.identity(model.keras_model.get_layer(\"input_anchors\").input)),\n",
    "    (\"input_anchors_output\", tf.identity(model.keras_model.get_layer(\"input_anchors\").output)),\n",
    "    \n",
    "    (\"input_image_meta_input\", tf.identity(model.keras_model.get_layer(\"input_image_meta\").input)),\n",
    "    (\"input_image_meta_output\", tf.identity(model.keras_model.get_layer(\"input_image_meta\").output)),\n",
    "    \n",
    "    (\"roi_align_classifier_output\", tf.identity(model.keras_model.get_layer(\"roi_align_classifier\").output)),\n",
    "    \n",
    "    (\"mrcnn_class_conv1\", tf.identity(model.keras_model.get_layer(\"mrcnn_class_conv1\").output)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposals are in normalized coordinates. Scale them\n",
    "# to image coordinates.\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n",
    "\n",
    "# Class ID, score, and mask per proposal\n",
    "roi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\n",
    "roi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\n",
    "# roi_class_names = np.array(dataset.class_names)[roi_class_ids]\n",
    "roi_class_names = np.array(class_names)[roi_class_ids]\n",
    "roi_positive_ixs = np.where(roi_class_ids > 0)[0]\n",
    "\n",
    "# How many ROIs vs empty rows?\n",
    "print(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\n",
    "print(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n",
    "\n",
    "# Class counts\n",
    "print(list(zip(*np.unique(roi_class_names, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of proposals.\n",
    "# Proposals classified as background are dotted, and\n",
    "# the rest show their class and confidence score.\n",
    "limit = 100\n",
    "# ixs = np.random.randint(0, proposals.shape[0], limit)\n",
    "ixs = np.where(roi_class_ids == 1)[0]\n",
    "print(len(ixs))\n",
    "# captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "#             for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n",
    "captions = [\"{} {:.3f}\".format(class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n",
    "visualize.draw_boxes(image, boxes=proposals[ixs],\n",
    "                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n",
    "                     captions=captions, title=\"ROIs Before Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Bounding Box Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-specific bounding box shifts.\n",
    "roi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\n",
    "log(\"roi_bbox_specific\", roi_bbox_specific)\n",
    "\n",
    "# Apply bounding box transformations\n",
    "# Shape: [N, (y1, x1, y2, x2)]\n",
    "refined_proposals = utils.apply_box_deltas(\n",
    "    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\n",
    "log(\"refined_proposals\", refined_proposals)\n",
    "\n",
    "# Show positive proposals\n",
    "# ids = np.arange(roi_boxes.shape[0])  # Display all\n",
    "limit = 100\n",
    "ids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\n",
    "# captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "#             for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\n",
    "captions = [\"{} {:.3f}\".format(class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\n",
    "visualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\n",
    "                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\n",
    "                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\n",
    "                     captions=captions, title=\"ROIs After Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Low Confidence Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove boxes classified as background\n",
    "keep = np.where(roi_class_ids > 0)[0]\n",
    "print(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low confidence detections\n",
    "keep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\n",
    "print(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\n",
    "    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-Class Non-Max Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply per-class non-max suppression\n",
    "pre_nms_boxes = refined_proposals[keep]\n",
    "pre_nms_scores = roi_scores[keep]\n",
    "pre_nms_class_ids = roi_class_ids[keep]\n",
    "\n",
    "nms_keep = []\n",
    "for class_id in np.unique(pre_nms_class_ids):\n",
    "    # Pick detections of this class\n",
    "    ixs = np.where(pre_nms_class_ids == class_id)[0]\n",
    "    # Apply NMS\n",
    "    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \n",
    "                                            pre_nms_scores[ixs],\n",
    "                                            config.DETECTION_NMS_THRESHOLD)\n",
    "    # Map indicies\n",
    "    class_keep = keep[ixs[class_keep]]\n",
    "    nms_keep = np.union1d(nms_keep, class_keep)\n",
    "#     print(\"{:22}: {} -> {}\".format(dataset.class_names[class_id][:20], \n",
    "#                                    keep[ixs], class_keep))\n",
    "    print(\"{:22}: {} -> {}\".format(class_names[class_id][:20], \n",
    "                                   keep[ixs], class_keep))\n",
    "\n",
    "keep = np.intersect1d(keep, nms_keep).astype(np.int32)\n",
    "print(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final detections\n",
    "ixs = np.arange(len(keep))  # Display all\n",
    "# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\n",
    "# captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "#             for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\n",
    "captions = [\"{} {:.3f}\".format(class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\n",
    "visualize.draw_boxes(\n",
    "    image, boxes=proposals[keep][ixs],\n",
    "    refined_boxes=refined_proposals[keep][ixs],\n",
    "    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\n",
    "    captions=captions, title=\"Detections after NMS\",\n",
    "    ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Generating Masks\n",
    "\n",
    "This stage takes the detections (refined bounding boxes and class IDs) from the previous layer and runs the mask head to generate segmentation masks for every instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Mask Targets\n",
    "\n",
    "These are the training targets for the mask branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Predicted Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of mask head\n",
    "mrcnn = model.run_graph([image], [\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
    "])\n",
    "\n",
    "# Get detection class IDs. Trim zero padding.\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\n",
    "det_class_ids = det_class_ids[:det_count]\n",
    "\n",
    "print(\"{} detections: {}\".format(\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks\n",
    "det_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\n",
    "det_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \n",
    "                              for i, c in enumerate(det_class_ids)])\n",
    "det_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\n",
    "                      for i, m in enumerate(det_mask_specific)])\n",
    "log(\"det_mask_specific\", det_mask_specific)\n",
    "log(\"det_masks\", det_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(det_mask_specific[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations\n",
    "\n",
    "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations of a few sample layers\n",
    "activations = model.run_graph([image], [\n",
    "    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n",
    "    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\n",
    "    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n",
    "    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image (normalized)\n",
    "_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Backbone feature map\n",
    "# 64 * 64 * 1024\n",
    "print(activations[\"res4w_out\"].shape)\n",
    "display_images(np.transpose(activations[\"res4w_out\"][0,:,:,:100], [2, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of RPN bounding box deltas\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title(\"dy\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,0], 50)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title(\"dx\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,1], 50)\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title(\"dw\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,2], 50)\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title(\"dh\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,3], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of y, x coordinates of generated proposals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"y1, x1\")\n",
    "plt.scatter(activations[\"roi\"][0,:,0], activations[\"roi\"][0,:,1])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"y2, x2\")\n",
    "plt.scatter(activations[\"roi\"][0,:,2], activations[\"roi\"][0,:,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
